# Configuration file for the NTL9 folding example using OpenMM

# The output directory for the runs
output_dir: runs/ntl9-v1

# The number of iterations to run the ensemble for
num_iterations: 106

# The basis states to use for the ensemble
basis_states:
  # The directory containing the basis states sub directories
  basis_state_dir: /nfs/lambda_stor_01/homes/abrace/projects/ddwe/src/deepdrivewe/examples/openmm_ntl9_hk/inputs
  # The file extension for the basis state files
  basis_state_ext: .pdb
  # The number of basis states to use
  initial_ensemble_members: 72
  # Whether to randomly initialize the ensemble members if there
  # are more basis state files than initial ensemble members
  randomly_initialize: true

# Strategy for initializing the basis state progress coordinates
basis_state_initializer:
  # The path to the reference PDB file
  reference_file: /nfs/lambda_stor_01/homes/abrace/projects/ddwe/src/deepdrivewe/examples/openmm_ntl9_hk/common_files/reference.pdb

# The configuration for the simulation
simulation_config:
  # The OpenMM configuration
  openmm_config:
    # The number of nanoseconds to run each simulation for (10 ps)
    simulation_length_ns: 0.01
    # How often to report frames in picoseconds
    report_interval_ps: 2.0
    # The time step to use in picoseconds
    dt_ps: 0.002
    # The temperature to run the simulation at
    temperature: 300.0
    # The solvent type
    solvent_type: implicit
    # The hardware platform to run the simulation on
    hardware_platform: CPU

  # The path to the reference PDB file
  reference_file: /nfs/lambda_stor_01/homes/abrace/projects/ddwe/src/deepdrivewe/examples/openmm_ntl9_hk/common_files/reference.pdb

# The configuration for training
train_config:
  # The path to the CVAE model configuration file
  config_path: /nfs/lambda_stor_01/homes/abrace/projects/ddwe/src/deepdrivewe/examples/openmm_ntl9_ddwe/cvae-config.yaml
  # The path to the CVAE model weights file
  checkpoint_path: /nfs/lambda_stor_01/homes/abrace/projects/ddwe/src/deepdrivewe/examples/openmm_ntl9_ddwe/checkpoint-epoch-100.pt

# The configuration for the inference
inference_config:
  # The path to the CVAE model configuration file
  ai_model_config_path: /nfs/lambda_stor_01/homes/abrace/projects/ddwe/src/deepdrivewe/examples/openmm_ntl9_ddwe/cvae-config.yaml
  # The path to the CVAE model weights file
  ai_model_checkpoint_path: /nfs/lambda_stor_01/homes/abrace/projects/ddwe/src/deepdrivewe/examples/openmm_ntl9_ddwe/checkpoint-epoch-100.pt

  # The number of neighbors to use for LOF
  lof_n_neighbors: 20
  # The distance metric to use for LOF [cosine, minkowski]
  lof_distance_metric: cosine

  # The number of simulations to maintain per bin (we only have one bin)
  # so this is the total number of simulations to maintain
  sims_per_bin: 72
  # The number of simulations to resample in each iteration
  consider_for_resampling: 36
  # The maximum number of resamples to perform in each iteration
  max_resamples: 8

# The target threshold for the progress coordinate
# to be considered in the target state.
target_states:
    - label: folded
      pcoord: [1.0]

# The settings for the compute environment
compute_config:
  # The name of the compute environment to use (CPU + GPU)
  name: inference_train_workstation

  # The CPU configuration for simulation
  cpu_config:
    # Specify we want the local parsl configuration
    name: local
    # The maximum number of worker processes to use for parallelization
    max_workers: 53

  # The GPU configuration for training
  train_gpu_config:
    # Specify we want the workstation parsl configuration
    name: workstation
    # Identify which GPUs to assign tasks to. It's generally recommended to first check
    # nvidia-smi to see which GPUs are available. The numbers below are analogous to
    # setting CUDA_VISIBLE_DEVICES=2
    available_accelerators: ["2"]

  # The GPU configuration for inference
  inference_gpu_config:
    # Specify we want the workstation parsl configuration
    name: workstation
    # Identify which GPUs to assign tasks to
    available_accelerators: ["3"]
